{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3547 TERM PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intelligent Systems and Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title: Robot obstacle avoidance with reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group Members:\n",
    "    1. Alexandre Dietrich\n",
    "    2. Ankur Tyagi\n",
    "    3. Haitham Alamri\n",
    "    4. Rodolfo Vasconcelos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date: August, 4 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introduction](#Introduction)\n",
    "* [Problem](#Problem)\n",
    "* [Solution](#Solution)\n",
    "    * [Alternative 1](#Alternative_1)\n",
    "        * [Training Phase](#Training_Phase)\n",
    "        * [Playing Phase](#Playing_Phase)\n",
    "    * [Alternative 2](#Alternative_2)\n",
    "        * [Description](#Description)\n",
    "        * [Evaluation](#Evaluation)\n",
    "* [Conclusion](#Conclusion)\n",
    "* [Appendix A](#Appendix_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Introduction'></a>\n",
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After ten weeks of learning additional Artificial Intelligence techniques, which cover more traditional branches of AI, such as research, planning, knowledge, logic and reinforcement learning, we had the opportunity to put what we learned into practice. After interesting brainstorming and group discussions, we decided to go ahead with a project using reinforcement learning. We found a challenging project and the right size to carry out a solution in a reasonable time and resources. We then put our efforts into defining the problem, defining workarounds, coding and testing our algorithms, evaluating the results and preparing for the conclusion. Here's what we did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Problem'></a>\n",
    "# Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement: find the optimal path for a robot to reach an end point, starting from anothe point, avoiding all obstacles during its journey and using reinforcement learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found an implementation of this challenge that uses other techniques, instead of reinforcement learning. Here is the link to the article that inspired us on this project. \n",
    "\n",
    "https://bayesianadventures.wordpress.com/2015/08/31/obstacle-avoidance-for-clever-robots/\n",
    "\n",
    "For this project, the environment created by the original simulator is called \"Robot World\".\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"original_simulator.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "<center><b>Figure 1. Original Robot World Simulator</b></center>\n",
    "<center><i>Source: Bayesian Adventures</i></center>\n",
    "</div> \n",
    "\n",
    "In this Robot World, you can move around by clicking on the step button. After a new step, the robot will be in a new state. You can add new obstacles during the journey dynamically. The objective is to reach the goal point (G) without colliding with an obstacle. You can also define the starting and goal points. \n",
    "\n",
    "The original code implemented three important classes: Robot, Sonar and Sonar_Array. The first one controls the robot, its position, its movements, etc. The other two implement the controls to obtain information from all the sensors and allows the robot to uderstantd if there are obstacles in close range, define the alerts and define the actions that control whether the robot should continue on its path or need to make a turn to avoid an obstacle. This part of the code is where our main problem lies. \n",
    "\n",
    "- Can we use reinforcement learning to define an optimal policy to guide the robot's actions to avoid the obstacles and reach the goal? \n",
    "- Can we replace the original algorithm by a more efficient RL algorithm or RL is not the best solution for this problem? \n",
    "- Can we extrapolate this problem to a drone or to a more dynamic environment ? \n",
    "\n",
    "These were some of the questions that guided our project. \n",
    "\n",
    "The original code and simultador can be see and run in this link: <br>\n",
    "http://www.codeskulptor.org/#user40_EEIxkOtKog_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     \n",
    "To effectively use the original code and simulator, we made some changes and adapted the Robot World. Our goal was to focus more on reinforcement learning algorithms than on other parts of the code. \n",
    "<div align=\"center\">\n",
    "<img src=\"new_simulator.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "<center><b>Figure 2. New Robot World Simulator</b></center>\n",
    "<center><i>Source: New Robot World from Term Project</i></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried different alternatives to solve the problem. To effectively use the original code and simulator, we made some changes and adapted the Robot World. Our main goal was to focus more on reinforcement learning algorithms than on other parts of the code, but we also adapted the original code to better compare the results of our alternatives. Below you can see our solution alternatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Alternative_1'></a>\n",
    "## Alternative 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first approach, we redefined the Robot World like this:\n",
    "\n",
    "New Robot World Assumptions: <br>\n",
    "500 x 500 pixels <br>\n",
    "Start point at the top left corner <br>\n",
    "End point at the bottom right corner <br>\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"new_simulator.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "<center><b>Figure 2. New Robot World Simulator</b></center>\n",
    "<center><i>Source: New Robot World from Term Project</i></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the definition of the Markov decision process: \n",
    "\n",
    "States: all possible states within a 4 x 4 grid with 0, 1 or 2 obstacles<br>\n",
    "Start State: top left corner<br>\n",
    "Actions(s): Up, Down, Right and left<br>\n",
    "T(s'|s; a): probability of s0 if take action a in state s = 1 (No uncertainty)<br>\n",
    "Reward(s; a; s0): -1 without obsctacle, -5 when there is an obstacle<br>\n",
    "End State: bottom right corner <br>\n",
    "Discount factor = 0.6<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Training_Phase'></a>\n",
    "### Training Phase\n",
    "\n",
    "\n",
    "To find an optimal policy to help our robot reach its goal without hitting obstacles, we made an operational decision and divided the New Robot World (500 x 500) in 100 4 x 4 position grids. With that, we created a strategy to find the best policy in a smaller grid, considering that we could have 15 different positions for one or two obstacles. We ran the Monte Carlo method to simulate thousands of episodes and value iteration to find the optimal q-value function for every state and action within the smaller grid to define the optimal policy (master policy). This same policy could be used for all other 99 grids. \n",
    "\n",
    "After that initial phase to define an optimal policy within this 4 x 4 grid, we just copied the print of policy to the code that implement the playing stage which was adapted to run the New Robot World. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert below the code that generates the Master Policy  - We need to decide how much of the code we want to show here ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert here any code to show Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Playing_Phase'></a>\n",
    "### Playing Phase\n",
    "\n",
    "With the Master Policy trained, we adapted the original code of the Robot World to create our New Robot World. We included the Master Policy as a dictionary considering States and Actions. We also created two functions: find_location_onMap and policy_finder, which are use to obtain information of the robot and obstacles within a specific grid and define the optimal action according the these positions. \n",
    "\n",
    "Within the code where the original program understands what action the robot should take, we changed the code to call our policy_finder fuction, which find the robot and obstacle locations, and return the action that the robot must take, according to our Master Policy. \n",
    "\n",
    "You can now play with the New Robot World by clicking on the step button and adding new obstacles. \n",
    "\n",
    "Here is the link to the New Robot World:\n",
    "\n",
    "http://www.codeskulptor.org/#user47_JJSgNIMtcq_0.py\n",
    "\n",
    "Following are parts of the code that were inserted or updated int the original simulator for the New Robot World."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert below the code that plays with the robot to avoid obstacles with RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# New Code\n",
    "# Add the master pilocy \n",
    "master_policy={0: {0: {(0, 0): 'R', (0, 1): 'R', (0, 2): 'R', (0, 3): 'D', (1, 0): 'D', (1, 1): 'L', (1, 2): 'U', (1, 3): 'D', (2, 0): 'R', (2, 1): 'D', (2, 2): 'D', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}, 1: {(0, 0): 'R', (0, 1): 'R', (0, 2): 'R', (0, 3): 'D', (1, 0): 'D', (1, 1): 'R', (1, 2): 'R', (1, 3): 'D', (2, 0): 'D', (2, 1): 'L', (2, 2): 'R', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}, 2: {(0, 0): 'R', (0, 1): 'R', (0, 2): 'D', (0, 3): 'D', (1, 0): 'U', (1, 1): 'R', (1, 2): 'R', (1, 3): 'D', (2, 0): 'R', (2, 1): 'D', (2, 2): 'R', (2, 3): 'D', (3, 0): 'U', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}, 3: {(0, 0): 'R', (0, 1): 'R', (0, 2): 'R', (0, 3): 'D', (1, 0): 'U', (1, 1): 'R', (1, 2): 'R', (1, 3): 'D', (2, 0): 'U', (2, 1): 'D', (2, 2): 'R', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}}, 1: {0: {(0, 0): 'R', (0, 1): 'R', (0, 2): 'R', (0, 3): 'D', (1, 0): 'R', (1, 1): 'D', (1, 2): 'R', (1, 3): 'D', (2, 0): 'D', (2, 1): 'D', (2, 2): 'D', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}, 1: {(0, 0): 'D', (0, 1): 'L', (0, 2): 'R', (0, 3): 'D', (1, 0): 'D', (1, 1): 'D', (1, 2): 'D', (1, 3): 'D', (2, 0): 'D', (2, 1): 'D', (2, 2): 'D', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}, 2: {(0, 0): 'D', (0, 1): 'L', (0, 2): 'L', (0, 3): 'D', (1, 0): 'D', (1, 1): 'D', (1, 2): 'R', (1, 3): 'D', (2, 0): 'R', (2, 1): 'D', (2, 2): 'L', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}, 3: {(0, 0): 'R', (0, 1): 'D', (0, 2): 'D', (0, 3): 'L', (1, 0): 'D', (1, 1): 'D', (1, 2): 'D', (1, 3): 'L', (2, 0): 'D', (2, 1): 'R', (2, 2): 'R', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}}, 2: {0: {(0, 0): 'R', (0, 1): 'R', (0, 2): 'D', (0, 3): 'D', (1, 0): 'R', (1, 1): 'R', (1, 2): 'D', (1, 3): 'D', (2, 0): 'D', (2, 1): 'R', (2, 2): 'R', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}, 1: {(0, 0): 'R', (0, 1): 'R', (0, 2): 'R', (0, 3): 'D', (1, 0): 'U', (1, 1): 'R', (1, 2): 'D', (1, 3): 'D', (2, 0): 'D', (2, 1): 'L', (2, 2): 'R', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}, 2: {(0, 0): 'R', (0, 1): 'R', (0, 2): 'R', (0, 3): 'D', (1, 0): 'D', (1, 1): 'U', (1, 2): 'R', (1, 3): 'D', (2, 0): 'D', (2, 1): 'D', (2, 2): 'R', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}, 3: {(0, 0): 'R', (0, 1): 'D', (0, 2): 'D', (0, 3): 'L', (1, 0): 'U', (1, 1): 'D', (1, 2): 'D', (1, 3): 'L', (2, 0): 'U', (2, 1): 'R', (2, 2): 'D', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}}, 3: {0: {(0, 0): 'R', (0, 1): 'R', (0, 2): 'D', (0, 3): 'D', (1, 0): 'U', (1, 1): 'U', (1, 2): 'D', (1, 3): 'D', (2, 0): 'U', (2, 1): 'D', (2, 2): 'R', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}, 1: {(0, 0): 'R', (0, 1): 'R', (0, 2): 'R', (0, 3): 'D', (1, 0): 'U', (1, 1): 'D', (1, 2): 'R', (1, 3): 'D', (2, 0): 'R', (2, 1): 'R', (2, 2): 'D', (2, 3): 'D', (3, 0): 'U', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}, 2: {(0, 0): 'R', (0, 1): 'R', (0, 2): 'D', (0, 3): 'D', (1, 0): 'R', (1, 1): 'R', (1, 2): 'R', (1, 3): 'D', (2, 0): 'U', (2, 1): 'U', (2, 2): 'R', (2, 3): 'D', (3, 0): 'R', (3, 1): 'U', (3, 2): 'R', (3, 3): 'U'}, 3: {(0, 0): 'D', (0, 1): 'D', (0, 2): 'R', (0, 3): 'D', (1, 0): 'R', (1, 1): 'R', (1, 2): 'R', (1, 3): 'D', (2, 0): 'U', (2, 1): 'D', (2, 2): 'D', (2, 3): 'D', (3, 0): 'U', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}}}\n",
    "master_policy_2obs={0: {0: {0: {0: {(0, 0): 'R', (0, 1): 'D', (0, 2): 'L', (0, 3): 'D', (1, 0): 'R', (1, 1): 'D', (1, 2): 'D', (1, 3): 'D', (2, 0): 'R', (2, 1): 'D', (2, 2): 'R', (2, 3): 'D',\n",
    "\n",
    "# New Code\n",
    "\n",
    "                                    \n",
    "# this function to find the location of agent or obtacles in the map \n",
    "# it convert 500X500 pixels word to 10x10 squars each with 12.5x12.5 pixxels\n",
    "def find_location_onMap(pos):\n",
    "  location_in_the_grid=[]\n",
    "  location_in_the_map=[]\n",
    "  i=0\n",
    "  i1=0\n",
    "  loc=0\n",
    "  loc1=0\n",
    "  for squares in range(0,10):\n",
    "    i1=0\n",
    "    for quare in range(0,10):\n",
    "      loc=0\n",
    "      #square.append((i,i1))\n",
    "      if ((pos[0]>i and pos[0]<=i+50) and (pos[1]>i1 and pos[1]<=i1+50)):\n",
    "        for rows in range(0,4):\n",
    "          loc1=0\n",
    "          for col in range(0,4):\n",
    "            if ((pos[0]>loc+i and pos[0]<=loc+i+12.5) and (pos[1]>loc1+i1 and pos[1]<=loc1+12.5+i1)):           \n",
    "              location_in_the_grid.append(loc)\n",
    "              location_in_the_grid.append(loc1)\n",
    "              location_in_the_map.append(i)\n",
    "              location_in_the_map.append(i1)\n",
    "            \n",
    "            loc1+=12.5\n",
    "          loc+=12.5\n",
    "      i1+=50\n",
    "    i+=50\n",
    "\n",
    "#To to convert to 4x4 grid each is 12.5 X 12.5 pixels\n",
    "  location_in_the_grid[0]=int(location_in_the_grid[0]/12.5)\n",
    "  location_in_the_grid[1]=int(location_in_the_grid[1]/12.5)\n",
    "#To to convert to 5x5 map each sqaure is 100X100 pixel\n",
    "  location_in_the_map[0]=int(location_in_the_map[0]/50)\n",
    "  location_in_the_map[1]=int(location_in_the_map[1]/50)\n",
    "  return location_in_the_map, location_in_the_grid\n",
    "\n",
    "# this function to get the policy and find the action fro the agen as per the obstacle location in 4x4 gris\n",
    "def policy_finder (mylocation,obs):\n",
    "  if(obs):\n",
    "    print(obs)\n",
    "    mylocation_onMap, my_location_onGrid = find_location_onMap(mylocation)\n",
    "    obs_location_onMap, obs_location_onGrid = find_location_onMap(obs[0])\n",
    "\n",
    "    policy= master_policy[obs_location_onGrid[0]][obs_location_onGrid[1]]\n",
    "    direction = policy.get((my_location_onGrid[0],my_location_onGrid[1]), ' ')\n",
    "    print(direction)\n",
    "    return direction\n",
    "\n",
    "def policy_finder2_obs (mylocation,obs):\n",
    "  mylocation_onMap, my_location_onGrid = find_location_onMap(mylocation)\n",
    "  obs_location_onMap, obs_location_onGrid = find_location_onMap(obs[0])\n",
    "  obs_location_onMap1, obs_location_onGrid1 = find_location_onMap(obs[1])\n",
    "  policy= master_policy_2obs[obs_location_onGrid[0]][obs_location_onGrid[1]][obs_location_onGrid1[0]][obs_location_onGrid1[1]]\n",
    "  direction = policy.get((my_location_onGrid[0],my_location_onGrid[1]), ' ')\n",
    "  print(\"kokokokoo\",direction)\n",
    "  return direction\n",
    "  \n",
    "# This function is to check if the obtacles and agent are in the same 12.5x12.5 pixels sqaure\n",
    "def check_obstacle(pos, obs_list):\n",
    "  obstacles=[]\n",
    "  location_on_map,location_on_grid = find_location_onMap(pos)\n",
    "  for i in obs_list:\n",
    "    location_on_map1,location_on_grid1 = find_location_onMap(i)\n",
    "    if (location_on_map1 == location_on_map):\n",
    "        obstacles.append(i)\n",
    "  return obstacles\n",
    "  return None\n",
    "                                    \n",
    "\n",
    "    def weighted_sum_method(self, robot_pos, robot_co):\n",
    "        #process data by the weighted sum method and \n",
    "        #return (1) whether turn is required or not (2) index of recommended sonar LOS to turn to\n",
    "        sum_d = 0\n",
    "        sum_wt = 0\n",
    "        alert = False\n",
    "        obs=[]\n",
    "        \n",
    "        obs=check_obstacle(robot_pos,full_obstacle_list)\n",
    "        print(\"---------------------------\")\n",
    "        print(\"I see obstacles\")\n",
    "        print(\"---------------------------\")\n",
    "        #print \"checking all sonars:\"      \n",
    "        for sonar in self.sonar_list:\n",
    "            #print \"sonar:\", sonar.index, \" range:\", sonar.output\n",
    "            if sonar.output < SENSOR_ALERT_R:#has this sonar found anything in danger zone?\n",
    "                alert = True\n",
    "                #print \"obstacle found by index \", sonar.index\n",
    "                for s1 in self.sonar_list: #process the whole array\n",
    "                    d = int(s1.output)\n",
    "                    gain = 1#SENSOR_MAX_R/(SENSOR_MAX_R - d)\n",
    "                    sum_d +=  d\n",
    "                    sum_wt += s1.index * d * gain\n",
    "                    #print \"I:\", s1.index,\",D:\",int(s1.output), \",sum_D:\", sum_d, \"sum_wt:\",sum_wt\n",
    "                rec_index = math.ceil(TURN_SCALE_FACTOR * float(sum_wt)/sum_d) #index of sonar with best LOS\n",
    "                #rec_index = int(TURN_SCALE_FACTOR * float(sum_d)/sum_wt)\n",
    "                print \"Rec index:\", rec_index\n",
    "                if abs(rec_index) > n_sensor/2:\n",
    "                    print \"rec index too large\"\n",
    "                    rec_index = n_sensor/2\n",
    "#                    if rec_index < 0:\n",
    "#                        rec_index = -n_sensor/2\n",
    "#                    else:\n",
    "#                        rec_index = n_sensor/2\n",
    "                print \"Rec index:\", rec_index \n",
    "                #break # processing completed\n",
    "            else: #no obstacle in danger zone\n",
    "                rec_index = 0\n",
    "                #return robot_co, False\n",
    "                #print : index = \", sonar.index\n",
    "        #print \"break from loop.\"\n",
    "        if rec_index == 0 and alert == True:\n",
    "                print \"alert with no alteration\"\n",
    "                obs=check_obstacle(robot_pos,full_obstacle_list)\n",
    "                print(obs)\n",
    "               \n",
    "                if (len(obs)!=0 and len(obs)<2):\n",
    "                        action = policy_finder(robot_pos,obs)\n",
    "                        I_was_here.append(robot_pos)\n",
    "                elif(len(obs)==2):\n",
    "                        action = policy_finder2_obs(robot_pos,obs)\n",
    "                        I_was_here.append(robot_pos)\n",
    "                else:\n",
    "                        return robot_co, False\n",
    "                if action:\n",
    "                        print(action,\"HAAH\")\n",
    "                        if action == 'R':\n",
    "                            print \"policy recommend to go right \"\n",
    "                            offset=90\n",
    "                        elif action == 'D':\n",
    "                            offset=180\n",
    "                            print \"policy recommend to go down \"\n",
    "                        elif action == 'L':\n",
    "                            offset=270\n",
    "                            print \"policy recommend to go left \"\n",
    "                        elif action == 'U':\n",
    "                            offset = 359\n",
    "                            print \"policy recommend to go down \"\n",
    "                        #if (I_was_here[-2]==robot_pos):\n",
    "                            #return robot_co+45, True\n",
    "                        print(\"Robot positon\",robot_pos)\n",
    "                        print(\"*********\")\n",
    "                        print(\"Robot Co\",robot_co)\n",
    "                        print(\"New Direction\",(offset%robot_co)+robot_co)\n",
    "                        print(\"******\")                        \n",
    "                        return offset, True\n",
    "                        #return (offset%robot_co)+robot_co, True \n",
    "        \n",
    "                        \n",
    "                \n",
    "        elif abs(rec_index) > 0: # some alteration recommended\n",
    "           obs=[]\n",
    "           offset =  rec_index * SENSOR_FOV #how much is the angular offset  \n",
    "           print(obs)\n",
    "           obs=check_obstacle(robot_pos,full_obstacle_list)\n",
    "           if(obs!=None):\n",
    "            \n",
    "                if (len(obs)!=0  and len(obs)<2):\n",
    "                            action = policy_finder(robot_pos,obs)\n",
    "                            I_was_here.append(robot_pos)\n",
    "                           \n",
    "                elif(len(obs)==2):\n",
    "                            action = policy_finder2_obs(robot_pos,obs)\n",
    "                            I_was_here.append(robot_pos)\n",
    "                            \n",
    "                else:\n",
    "                            return robot_co, False\n",
    "                if action:\n",
    "                            if action == 'R':\n",
    "                                print \"policy recommend to go right \"\n",
    "                                offset=90\n",
    "                            elif action == 'D':\n",
    "                                offset=180\n",
    "                                print \"policy recommend to go down \"\n",
    "                            elif action == 'L':\n",
    "                                offset=270\n",
    "                                print \"policy recommend to go left \"\n",
    "                            elif action == 'U':\n",
    "                                offset = 359\n",
    "                                print \"policy recommend to go down \"\n",
    "                            #if (I_was_here[-2]==robot_pos):\n",
    "                                    #return robot_co+45, True\n",
    "                print(\"Robot positon\",robot_pos)\n",
    "                #print(\"obs positon\",obs)\n",
    "                print(\"*********\")\n",
    "                print(\"Robot Co\",robot_co)\n",
    "                print(\"New Direction\",(offset%robot_co)+robot_co)\n",
    "                #print(\"location in the grid:\",find_location_onMap(robot_pos))\n",
    "                #print(\"location in the grid:\",find_location_onMap(obs[0]))\n",
    "                print(\"******\")\n",
    "                return offset,True\n",
    "                #return (offset%robot_co)+robot_co, True\n",
    "           else:     \n",
    "            \n",
    "                return (offset%robot_co), True\n",
    "        else:# no diversion needed\n",
    "           obs=[]\n",
    "           offset =  rec_index * SENSOR_FOV #how much is the angular offset  \n",
    "           print(obs)\n",
    "           obs=check_obstacle(robot_pos,full_obstacle_list)\n",
    "           if(obs!=None):\n",
    "            \n",
    "                if (len(obs)!=0  and len(obs)<2):\n",
    "                            action = policy_finder(robot_pos,obs)\n",
    "                            I_was_here.append(robot_pos)\n",
    "                            print(\"chinca:1\")\n",
    "                elif(len(obs)==2):\n",
    "                            action = policy_finder2_obs(robot_pos,obs)\n",
    "                            I_was_here.append(robot_pos)\n",
    "                            print(\"chinca:2\")\n",
    "                else:\n",
    "                            return robot_co, False\n",
    "                if action:\n",
    "                            if action == 'R':\n",
    "                                print \"policy recommend to go right \"\n",
    "                                offset=90\n",
    "                            elif action == 'D':\n",
    "                                offset=180\n",
    "                                print \"policy recommend to go down \"\n",
    "                            elif action == 'L':\n",
    "                                offset=270\n",
    "                                print \"policy recommend to go left \"\n",
    "                            elif action == 'U':\n",
    "                                offset = 359\n",
    "                                print \"policy recommend to go down \"\n",
    "                            #if (I_was_here[-2]==robot_pos):\n",
    "                                    #return robot_co+45, True\n",
    "                print(\"Robot positon\",robot_pos)\n",
    "                #print(\"obs positon\",obs)\n",
    "                print(\"*********\")\n",
    "                print(\"Robot Co\",robot_co)\n",
    "                print(\"New Direction\",(offset%robot_co)+robot_co)\n",
    "                #print(\"location in the grid:\",find_location_onMap(robot_pos))\n",
    "                #print(\"location in the grid:\",find_location_onMap(obs[0]))\n",
    "                print(\"******\")\n",
    "                return offset,True\n",
    "                #return (offset%robot_co)+robot_co, True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of this alternative were not good enough. We had too many assumptions like fix start and end points, no more than two obstacles per 4 x 4 grids, two different phases (training and playing) and the could only play the game online clicking on the step button.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Alternative_2'></a>\n",
    "## Alternative 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our second alternative, we decided to keep the same strategy as alternative 1 of dividing the original world of 500 x 500 into 100 4 x 4 grids. In this alternative, we allowed dynamic start and end points and no fix number of obstacles within a 4 x 4 grid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Description'></a>\n",
    "### Description\n",
    "\n",
    "Following is the definition of the Markov decision process: \n",
    "\n",
    "States: all possible states within a 4 x 4 grid with 0 to 15 obstacles<br>\n",
    "Start State: dynamic, defined when the episode starts<br>\n",
    "Actions(s): Up, Down, Right and left<br>\n",
    "T(s'|s; a): probability of s0 if take action a in state s = 1 (No uncertainty)<br>\n",
    "Reward(s; a; s0): -1 without obsctacle, -5 when there is an obstacle<br>\n",
    "End State: dynamic, defined when the episode starts <br>\n",
    "Discount factor = 0.6<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the performance to the original code and our solution, we converted the original code into Python 3, broked it in 4 Python programs to implement the three classes (Robot, Sonar and Sonar_Array) and utilitarian functions. This was important to run multiples episodes of the Robot in a bacth mode. \n",
    "\n",
    "Our reinforcement learning strategy is now considering a dynamic policy. We implemented just one phase (playing), starting every episode without a policy. For every step (action) taken by our robot in the New Robot World, if a policy is not available for that state, a new policy is created using the same Monte Carlo method used in the alternative 1. Then the policy for that state is  stored. If the robot enters a state where a possible had been previously defined, then the robot uses that policy. The array with the policies is being appended as long as the robot moves step by step until it reaches the goal. \n",
    "\n",
    "Our new robot, supported now by reinforcement leanirng instead of bayesian methods, can run in the same graphical interface than the original robot. However, to compare the performance of the two robots, we create a code to run both robots simultaneous, with the same number and position of obstacles, and to capture the number of steps each robot takes to reach the goal after 200 episodes. Then, we can plot two graphs to compare the accuracy and effort of both robots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert below the code for alternatives: different RL methods, new assuptions, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Monte Carlo Method\n",
    "\n",
    "\"\"\"## Run all episodes\"\"\"\n",
    "def calculate_gridworld_policy(end_state=(3,3),obstable_list = []):\n",
    "  # use the standard grid again (0 for every step) so that we can compare\n",
    "  # to iterative policy evaluation\n",
    "  # grid = standard_grid()\n",
    "  # try the negative grid too, to see if agent will learn to go past the \"bad spot\"\n",
    "  # in order to minimize number of steps\n",
    "  grid = negative_grid(step_cost=-1)\n",
    "  update_rewards(grid, obstable_list, -5)\n",
    "  update_end_state(grid, end_state)\n",
    "\n",
    "  # print rewards\n",
    "  #print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "\n",
    "  pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))  # probability of action (def random)\n",
    "\n",
    "  # state -> action\n",
    "  # initialize a random policy\n",
    "  policy = {}\n",
    "  for s in grid.actions.keys():\n",
    "      policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "  # initialize Q(s,a) and returns\n",
    "  Q = {}\n",
    "  returns = {} # dictionary of state -> list of returns we've received\n",
    "  states = grid.all_states()\n",
    "  for s in states:\n",
    "      if s in grid.actions: # not a terminal state\n",
    "        Q[s] = {}\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "          Q[s][a] = -10\n",
    "          returns[(s,a)] = []\n",
    "  else:\n",
    "      # terminal state or state we can't otherwise get to\n",
    "      pass\n",
    "\n",
    "  #print(\"initial Q:\")\n",
    "  #print_Q(Q,grid)\n",
    "\n",
    "  # repeat until convergence\n",
    "  deltas = []\n",
    "  for t in range(EPISODES):\n",
    "      #if t % 1000 == 0:\n",
    "          #print(t)\n",
    "          #print(\"Q:\")\n",
    "          #print_Q(Q,grid)\n",
    "\n",
    "      # generate an episode using pi\n",
    "      biggest_change = 0\n",
    "      states_actions_returns = play_episode(grid, policy, pi)\n",
    "\n",
    "      # calculate Q(s,a)\n",
    "      seen_state_action_pairs = set()\n",
    "      for s, a, G in states_actions_returns:\n",
    "          # check if we have already seen s\n",
    "          # called \"first-visit\" MC policy evaluation\n",
    "          sa = (s, a)\n",
    "          if sa not in seen_state_action_pairs:\n",
    "              old_q = Q[s][a]\n",
    "              returns[sa].append(G)\n",
    "              Q[s][a] = np.mean(returns[sa])\n",
    "              biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
    "              seen_state_action_pairs.add(sa)\n",
    "              A_star, _ = max_dict(Q[s])\n",
    "              for a_index in ALL_POSSIBLE_ACTIONS:\n",
    "                  if a_index == A_star:   pi[(s,a_index)] = 1 - EPS + EPS/len(ALL_POSSIBLE_ACTIONS)\n",
    "                  else:                   pi[(s,a_index)] = EPS/len(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "      deltas.append(biggest_change)\n",
    "\n",
    "      # calculate new policy pi(s) = argmax[a]{ Q(s,a) }\n",
    "      for s in policy.keys():\n",
    "          a, _ = max_dict(Q[s])\n",
    "          policy[s] = a\n",
    "\n",
    "  \"\"\"## Print results\"\"\"\n",
    "\n",
    "  #plt.plot(deltas)\n",
    "  #plt.show()\n",
    "\n",
    "  # find the optimal state-value function\n",
    "  # V(s) = max[a]{ Q(s,a) }\n",
    "  V = {}\n",
    "  for s in policy.keys():\n",
    "      V[s] = max_dict(Q[s])[1]\n",
    "\n",
    "  #print(\"final values:\")\n",
    "  #print_values(V, grid)\n",
    "  #print(\"final policy:\")\n",
    "  #print_policy(policy, grid)\n",
    "  #print(\"final Q:\")\n",
    "  #print_Q(Q,grid)\n",
    "  return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate a policy or use one that is already calculated\n",
    "\n",
    "def dynamic_policy_finder (mylocation, obs, master_policy, goal_pos):\n",
    "    print(f\"mylocation={mylocation}, obs={obs}, goal_pos={goal_pos}\")\n",
    "    \n",
    "    mylocation_onMap, my_location_onGrid = find_location_onMap(mylocation)\n",
    "    print(f\"mylocation_onMap={mylocation_onMap}, my_location_onGrid={my_location_onGrid}\")\n",
    "\n",
    "    obs_location_onGrid_array = []\n",
    "\n",
    "    for obstacle_pos in obs:\n",
    "        #_, obs_location_onGrid = find_location_onMap(obstacle_pos)\n",
    "        #obs_location_onGrid_array.append((obs_location_onGrid[0],obs_location_onGrid[1]))\n",
    "        obs_location_onGrid_array.extend(calculate_obstacle_onGrid(obstacle_pos))\n",
    "        print(f\"obs_location_onGrid_array={obs_location_onGrid_array}\")\n",
    "    \n",
    "    end_state = calculate_end_state_onGrid(mylocation, obs_location_onGrid_array, goal_pos)\n",
    "\n",
    "    policy_key = f\"{end_state}|{obs_location_onGrid_array}\"\n",
    "    print(f\"policy_key={policy_key}\")\n",
    "\n",
    "    if policy_key in master_policy:\n",
    "        policy = master_policy[policy_key]\n",
    "    else:\n",
    "        policy = montecarlo.calculate_gridworld_policy(end_state, obs_location_onGrid_array)\n",
    "        master_policy[policy_key] = policy\n",
    "\n",
    "    #policy= master_policy[obs_location_onGrid[0]][obs_location_onGrid[1]]\n",
    "    direction = policy.get((my_location_onGrid[0],my_location_onGrid[1]), ' ')\n",
    "    print(f\"direction={direction}\")\n",
    "    return direction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Class Sonar_Array\n",
    "\n",
    "    def weighted_sum_method(self, robot_pos, robot_co,full_obstacle_list,master_policy,I_was_here,goal_pos)\n",
    "\n",
    "        print (f\"weighted_sum_method, robot_pos={robot_pos}, robot_co={robot_co}\")\n",
    "\n",
    "        sum_d = 0\n",
    "        sum_wt = 0\n",
    "        alert = False\n",
    "        obs=[]\n",
    "        \n",
    "        obs=check_obstacle(robot_pos,full_obstacle_list)\n",
    "               \n",
    "        action = utils.dynamic_policy_finder(robot_pos,obs,master_policy,goal_pos)\n",
    "        print(f\"action={action},\")\n",
    "        if action == 'R':\n",
    "            print (\"policy recommend to go right \")\n",
    "            offset=90\n",
    "        elif action == 'D':\n",
    "            offset=180\n",
    "            print (\"policy recommend to go down \")\n",
    "        elif action == 'L':\n",
    "            offset=270\n",
    "            print (\"policy recommend to go left \")\n",
    "        elif action == 'U':\n",
    "            offset = 359\n",
    "            print (\"policy recommend to go down \")\n",
    "        else:\n",
    "            offset = 0\n",
    "            print (\"no policy?\")\n",
    "        #if (I_was_here[-2]==robot_pos):\n",
    "            #return robot_co+45, True\n",
    "        print(\"Robot positon\",robot_pos)\n",
    "        print(\"*********\")\n",
    "        print(\"Robot Co\",robot_co)\n",
    "        print(\"New Direction\",(offset%robot_co)+robot_co)\n",
    "        print(\"******\")                        \n",
    "        return offset, True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the aspects of the performance of the algoritms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"WhatsApp Image 2020-08-01 at 13.30.50.jpeg\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "<center><b>Figure 4. Performance of both Robots</b></center>\n",
    "<center><i>Source: Term Project</i></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"robot_movements.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "<center><b>Figure 3. Final Robot Movements with RL</b></center>\n",
    "<center><i>Source: XXXXXX</i></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Appendix_A'></a>\n",
    "# Appendix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the code prepared for this project is available on Github. Here is the link for the project. \n",
    "\n",
    "https://github.com/ravasconcelos/rl_obstacle_avoidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python UWaterloo",
   "language": "python",
   "name": "uwaterloo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
