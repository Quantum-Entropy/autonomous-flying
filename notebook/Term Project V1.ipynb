{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3547 TERM PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intelligent Systems and Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title: Robot obstacle avoidance with reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group Members:\n",
    "    1. Alexandre Dietrich\n",
    "    2. Ankur Tyagi\n",
    "    3. Haitham Alamri\n",
    "    4. Rodolfo Vasconcelos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date: August, 4 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Introduction](#Introduction)\n",
    "* [Problem](#Problem)\n",
    "* [Solution](#Solution)\n",
    "    * [Training Phase](#Training_Phase)\n",
    "    * [Playing Phase](#Playing_Phase)\n",
    "* [Alternatives](#Alternatives)\n",
    "* [Evaluation](#Evaluation)\n",
    "* [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Introduction'></a>\n",
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After ten weeks learning additional Artificial Intelligence techniques that cover more traditional AI branchs like search, planning, knowledge, logic and reinforcement learning, we had an opportunity to put what we have learned in practices. After interesting brainstorming and debates, we decided to go forward with a project using reinforcement learning. After finding an interesting problem and with the right size to accomplish a solution in a razonable time and resources, we put our efforts to define the problem, define alternatives solutions, code and test our algoritms, evaluate the results and prepare the conclusion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Problem'></a>\n",
    "# Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement: find the optimal path for a robot to reach point B starting from point A, avoiding all obstacles during its journey, using reinforcement learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found an implementation of this problem which uses other techniques instead of RL. We called it the \"Robot World\".\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"original_simulator.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "<center><b>Figure 1. Original Robot World Simulator</b></center>\n",
    "<center><i>Source: XXXXXX</i></center>\n",
    "</div> \n",
    "\n",
    "In this robot world, you can move by clicking on the step button. After a new step, the robot will be in a new state. You can include new obstacles during the journey dynamically. The objective is to reach the goal point (G) without bumbing in an obstacle. You can also define the starting and goal points. \n",
    "\n",
    "The original code implemented three important classes: Robot, Sonar and Sonar_Array. The first one controls the robot, is position, its moves, etc. The other two implement the controls to get the information from the all the sensors to uderstantd if there are obstacles in a close position, define the alerts and define the actions that controlif the robot should contunue in its trajectory or need to make a turn to avoid the obstacle. This part of the code is where our main problem resides. How can we use reinforcement learning to define an optimal policy to guide the actions to avoid the obstacles and reach the goal ?  \n",
    "\n",
    "The original code and simultador can be see and run in this link: <br>\n",
    "http://www.codeskulptor.org/#user40_EEIxkOtKog_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To effectviley use the original code and simulator, we made some changes and adapted the Robot World. Our objective was to focus more in the reinforcement learning than in other parts of the code. Here are the main changes:\n",
    "\n",
    "New Robot World Assuptions: <br>\n",
    "500 x 500 pyxels <br>\n",
    "Start point at the top left corner <br>\n",
    "End point at the bottom right corner <br>\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"new_simulator.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "<center><b>Figure 2. New Robot World Simulator</b></center>\n",
    "<center><i>Source: XXXXXX</i></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the definition of the Markov decision process: \n",
    "\n",
    "States: the set of states  (XXXXXXX)<br>\n",
    "Start State: top left corner<br>\n",
    "Actions(s): Up, Down, Right and left<br>\n",
    "T(s'|s; a): probability of s0 if take action a in state s = 1 (No uncertainty)<br>\n",
    "Reward(s; a; s0): reward for the transition (s; a; s0) = XXXXXX<br>\n",
    "End State: bottom right corner <br>\n",
    "Discount factor = 0.7<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our solution departed from the assuptions we have made to adapt to a new Robot World. In our journey to find an optimal policy to help our robot to achieve its goal without bumping in obstacles, we took an operational decision and divided the New Robot World (500 x 500) in 100 grids of 4 x 4 positions. With that, we created a strategy to find the best policy inside a smaller grid. After finding the policy which provide the robot to a optimal path inside this 4 x 4 grid, we can use it to any other grid inside the New Robot World. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Training_Phase'></a>\n",
    "## Training Phase\n",
    "\n",
    "To build the master policy, we use Monte Carlo algorithm to simulate thousands of episodes and value iteration to find the optimal q-value fucntion for every state and action. \n",
    "\n",
    "With the Master Policy trained, we adapted the original code of the Robot World to our create our New Robot World. We included the Master Policy as a dictionary considering States and Actions. We also created to functions: find_location_onMap and policy_finder, which are use to get information of the robot and obstacles inside an specific grid and define the optimal actions according the these positions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert below the code that generates the Master Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "master_policy={}\n",
    "\n",
    "class Grid: # Environment\n",
    "  def __init__(self, width, height, start):\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "    self.i = start[0]\n",
    "    self.j = start[1]\n",
    "\n",
    "  def set(self, rewards, actions):\n",
    "    # rewards should be a dict of: (i, j): r (row, col): reward\n",
    "    # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "    self.rewards = rewards\n",
    "    self.actions = actions\n",
    "\n",
    "  def set_state(self, s):\n",
    "    self.i = s[0]\n",
    "    self.j = s[1]\n",
    "\n",
    "  def current_state(self):\n",
    "    return (self.i, self.j)\n",
    "\n",
    "  def is_terminal(self, s):\n",
    "    return s not in self.actions\n",
    "\n",
    "  def move(self, action):\n",
    "    # check if legal move first\n",
    "    if action in self.actions[(self.i, self.j)]:\n",
    "      same_state = (self.i,self.j)\n",
    "      if action == 'U':\n",
    "        self.i -= 1\n",
    "      elif action == 'D':\n",
    "        self.i += 1\n",
    "      elif action == 'R':\n",
    "        self.j += 1\n",
    "      elif action == 'L':\n",
    "        self.j -= 1\n",
    "    # # if agent hit the wall go back to same position.\n",
    "    if (self.i==4 or self.i==-1):\n",
    "      (self.i,self.j)=same_state\n",
    "    if (self.j==4 or self.j==-1):\n",
    "      (self.i,self.j)=same_state\n",
    "        # return a reward (if any)\n",
    "    return self.rewards.get((self.i, self.j), 0)\n",
    "\n",
    "  def game_over(self):\n",
    "    # returns true if game is over, else false\n",
    "    # true if we are in end state (0,0) or (3,3)\n",
    "    end_state = [(3,3)]\n",
    "    return (self.i, self.j) in end_state\n",
    "\n",
    "  def all_states(self):\n",
    "    # either a position that has possible next actions\n",
    "    # or a position that yields a reward\n",
    "    return set(self.actions.keys()) | set(self.rewards.keys())\n",
    "\n",
    "\n",
    "def standard_grid(starting_position):\n",
    "  # define a grid that describes the reward for arriving at each state\n",
    "  # and possible actions at each state\n",
    "  g = Grid(4, 4, starting_position)\n",
    "  rewards = {\n",
    "    (3, 3): 0,\n",
    "    (k1,k2):-5,\n",
    "  }\n",
    "  actions = {\n",
    "    (0, 0): ('L', 'D', 'R','U'),\n",
    "    (0, 1): ('L', 'D', 'R','U'),\n",
    "    (0, 2): ('L', 'D', 'R','U'),\n",
    "    (0, 3): ('L', 'D', 'R','U'),\n",
    "    (1, 0): ('L', 'D', 'R','U'),\n",
    "    (1, 1): ('L', 'D', 'R','U'),\n",
    "    (1, 2): ('L', 'D', 'R','U'),\n",
    "    (1, 3): ('L', 'D', 'R','U'),\n",
    "    (2, 0): ('L', 'D', 'R','U'),\n",
    "    (2, 1): ('L', 'D', 'R','U'),\n",
    "    (2, 2): ('L', 'D', 'R','U'),\n",
    "    (2, 3): ('L', 'D', 'R','U'),\n",
    "    (3, 0): ('L', 'D', 'R','U'),\n",
    "    (3, 1): ('L', 'D', 'R','U'),\n",
    "    (3, 2): ('L', 'D', 'R','U'),\n",
    "    (3, 3): ('L', 'D', 'R','U'),\n",
    "  }\n",
    "  g.set(rewards, actions)\n",
    "  return g\n",
    "\n",
    "\n",
    "def negative_grid(step_cost=-1):\n",
    "  # we minimize the number of moves\n",
    "  # so we will penalize every move (-1) except for the end-state (0)\n",
    "  \n",
    "  states = [[i, j] for i in range(4) for j in range(4)]\n",
    "  initState = random.choice(states[1:-1])\n",
    "  g = standard_grid(initState)\n",
    "  g.rewards.update({\n",
    "    (0, 0): step_cost,\n",
    "    (0, 1): step_cost,\n",
    "    (0, 2): step_cost,\n",
    "    (0, 3): step_cost,\n",
    "    (1, 0): step_cost,\n",
    "    (1, 1): step_cost,\n",
    "    (1, 2): step_cost,\n",
    "    (1, 3): step_cost,\n",
    "    (2, 0): step_cost,\n",
    "    (2, 1): step_cost,\n",
    "    (2, 2): step_cost,\n",
    "    (2, 3): step_cost,\n",
    "    (3, 0): step_cost,\n",
    "    (3, 1): step_cost,\n",
    "    (3, 2): step_cost,\n",
    "    (3, 3): 0,\n",
    "    (k1,k2): -5,\n",
    "\n",
    "  })\n",
    "  return g\n",
    "\n",
    "\n",
    "def print_values(V, g):\n",
    "  for i in range(g.width):\n",
    "    print(\"---------------------------\")\n",
    "    for j in range(g.height):\n",
    "      v = V.get((i,j), 0)\n",
    "      if v >= 0:\n",
    "        print(\" %.2f|\" % v, end=\"\")\n",
    "      else:\n",
    "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "def print_policy(P, g):\n",
    "  for i in range(g.width):\n",
    "    print(\"---------------------------\")\n",
    "    for j in range(g.height):\n",
    "      a = P.get((i,j), ' ')\n",
    "      print(\"  %s  |\" % a, end=\"\")\n",
    "    print(\"\")\n",
    "# the Monte Carlo Epsilon-Greedy method to find the optimal policy and value function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "GAMMA = 0.7\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "def random_action(a, eps=0.1):\n",
    "  # choose given a with probability 1 - eps + eps/4\n",
    "  p = np.random.random()\n",
    "  if p < (1 - eps):\n",
    "    return a\n",
    "  else:\n",
    "    return np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "def max_dict(d):\n",
    "  # returns the argmax (key) and max (value) from a dictionary\n",
    "  max_key = None\n",
    "  max_val = float('-inf')\n",
    "  for k, v in d.items():\n",
    "    if v > max_val:\n",
    "      max_val = v\n",
    "      max_key = k\n",
    "  return max_key, max_val\n",
    "\n",
    "def play_game(grid, policy):\n",
    "  # returns a list of states and corresponding returns\n",
    "  # use an epsilon-soft policy\n",
    "  # Random start position\n",
    "  states = [(i, j) for i in range(4) for j in range(4)]\n",
    "  initState = random.choice(states[1:-1])\n",
    "  g = standard_grid(initState)\n",
    "  s = initState\n",
    "  grid.set_state(s)\n",
    "  a = random_action(policy[s])\n",
    "\n",
    "  # each triple is s(t), a(t), r(t)\n",
    "  # but r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
    "  states_actions_rewards = [(s, a, 0)]\n",
    "  while True:\n",
    "    r = grid.move(a)\n",
    "    s = grid.current_state()\n",
    "    if grid.game_over():\n",
    "      states_actions_rewards.append((s, None, r))\n",
    "      break\n",
    "    else:\n",
    "      a = random_action(policy[s]) # the next state is stochastic\n",
    "      states_actions_rewards.append((s, a, r))\n",
    "\n",
    "  # calculate the returns by working backwards from the terminal state\n",
    "  G = 0\n",
    "  states_actions_returns = []\n",
    "  first = True\n",
    "  for s, a, r in reversed(states_actions_rewards):\n",
    "    # the value of the terminal state is 0 \n",
    "    # we should ignore the first state we encounter\n",
    "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "    if first:\n",
    "      first = False\n",
    "    else:\n",
    "      states_actions_returns.append((s, a, G))\n",
    "    G = r + GAMMA*G\n",
    "  states_actions_returns.reverse() # we want it to be in order of state visited\n",
    "  return states_actions_returns\n",
    "\n",
    "for i in range(0,4):\n",
    "  master_policy[i]={}\n",
    "  for i1 in range(0,4):\n",
    "    if (i==0 & i1==0):\n",
    "      k1=2\n",
    "      k2=2\n",
    "    elif (i==3 & i1==3):\n",
    "      k1=2\n",
    "      k2=2\n",
    "    else:\n",
    "      k1=i\n",
    "      k2=i1\n",
    "    grid = negative_grid(step_cost=-1)\n",
    "# print rewards\n",
    "    print(\"rewards:\")\n",
    "    print_values(grid.rewards, grid)\n",
    "\n",
    "# state -> action\n",
    "# initialize a random policy\n",
    "    policy = {}\n",
    "    for s in grid.actions.keys():\n",
    "      policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "  \n",
    "# initial policy\n",
    "    print(\"initial policy:\")\n",
    "    print_policy(policy, grid)\n",
    "\n",
    "# initialize Q(s,a) and returns\n",
    "    Q = {}\n",
    "    returns = {} # dictionary of state -> list of returns we've received\n",
    "    states = grid.all_states()\n",
    "    for s in states:\n",
    "      if s in grid.actions: # not a terminal state\n",
    "        Q[s] = {}\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "          Q[s][a] = 0\n",
    "          returns[(s,a)] = []\n",
    "      else:\n",
    "    # terminal state or state we can't otherwise get to\n",
    "        pass\n",
    "  \n",
    "# initial Q values for all states in grid\n",
    "    print(Q)\n",
    "    print(s)\n",
    "\n",
    "\n",
    "\n",
    "# repeat\n",
    "    deltas = []\n",
    "    for t in range(1000):\n",
    "  # generate an episode using pi\n",
    "      biggest_change = 0\n",
    "      states_actions_returns = play_game(grid, policy)\n",
    "\n",
    "  # calculate Q(s,a)\n",
    "      seen_state_action_pairs = set()\n",
    "      for s, a, G in states_actions_returns:\n",
    "    # check if we have already seen s\n",
    "    # called \"first-visit\" MC policy evaluation\n",
    "        sa = (s, a)\n",
    "        if sa not in seen_state_action_pairs:\n",
    "          old_q = Q[s][a]\n",
    "          returns[sa].append(G)\n",
    "          Q[s][a] = np.mean(returns[sa])\n",
    "          biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
    "          seen_state_action_pairs.add(sa)\n",
    "      deltas.append(biggest_change)\n",
    "\n",
    "  # calculate new policy pi(s) = argmax[a]{ Q(s,a) }\n",
    "      for s in policy.keys():\n",
    "        a, _ = max_dict(Q[s])\n",
    "        policy[s] = a\n",
    "\n",
    "\n",
    "    plt.plot(deltas)\n",
    "    plt.show()\n",
    "\n",
    "# find the optimal state-value function\n",
    "# V(s) = max[a]{ Q(s,a) }\n",
    "    V = {}\n",
    "    for s in policy.keys():\n",
    "      V[s] = max_dict(Q[s])[1]\n",
    "\n",
    "# Print the table of the estimated function Q(s,a) for the optimal policy \n",
    "    print(\"final values:\")\n",
    "    print_values(V, grid)\n",
    "\n",
    "    print(\"final policy:\")\n",
    "# Print the policy pi(s) \n",
    "    print_policy(policy, grid)\n",
    "    print(policy)\n",
    "    master_policy[i][i1]={}\n",
    "    master_policy[i][i1]=policy\n",
    "    print(\"#########################\")\n",
    "    print(i,i1)\n",
    "    print(\"#######################\")\n",
    "\n",
    "print (master_policy)\n",
    "\n",
    "def find_location_onMap(pos):\n",
    "  location_in_the_grid=[]\n",
    "  location_in_the_map=[]\n",
    "  i=0\n",
    "  i1=0\n",
    "  loc=0\n",
    "  loc1=0\n",
    "  for squares in range(0,10):\n",
    "    i1=0\n",
    "    for quare in range(0,10):\n",
    "      loc=0\n",
    "      #square.append((i,i1))\n",
    "      if ((pos[0]>i and pos[0]<=i+50) and (pos[1]>i1 and pos[1]<=i1+50)):\n",
    "        for rows in range(0,4):\n",
    "          loc1=0\n",
    "          for col in range(0,4):\n",
    "            if ((pos[0]>loc+i and pos[0]<=loc+i+12.5) and (pos[1]>loc1+i1 and pos[1]<=loc1+12.5+i1)):           \n",
    "              location_in_the_grid.append(loc)\n",
    "              location_in_the_grid.append(loc1)\n",
    "              location_in_the_map.append(i)\n",
    "              location_in_the_map.append(i1)\n",
    "            \n",
    "            loc1+=12.5\n",
    "          loc+=12.5\n",
    "      i1+=50\n",
    "    i+=50\n",
    "\n",
    "#To to convert to 3x3 grid\n",
    "  location_in_the_grid[0]=int(location_in_the_grid[0]/12.5)\n",
    "  location_in_the_grid[1]=int(location_in_the_grid[1]/12.5)\n",
    "#To to convert to 5x5 map each sqaure is 100X100 pixel\n",
    "  location_in_the_map[0]=int(location_in_the_map[0]/50)\n",
    "  location_in_the_map[1]=int(location_in_the_map[1]/50)\n",
    "  return location_in_the_map, location_in_the_grid\n",
    "\n",
    "def policy_finder (mylocation,obs):\n",
    "    policy= master_policy[obs_location_onGrid[0]][obs_location_onGrid[1]]\n",
    "    direction = policy.get((my_location_onGrid[0],my_location_onGrid[1]), ' ')\n",
    "    print(direction)\n",
    "    return direction\n",
    "  \n",
    "\n",
    "\n",
    "# This function is to check if the obtacles and agent are in the same 12.5x12.5 pixels sqaure\n",
    "def check_obstacle(obs, obs_list):\n",
    "  location_on_map,location_on_grid = find_location_onMap(obs)\n",
    "  print(location_on_map)\n",
    "  for i in full_obstacle_list:\n",
    "    location_on_map1,location_on_grid1 = find_location_onMap(i)\n",
    "    if (location_on_map1 == location_on_map):\n",
    "        return location_on_map\n",
    "  return None\n",
    "\n",
    "# Master policy for 2 Obstacles\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "master_policy={}\n",
    "\n",
    "class Grid: # Environment\n",
    "  def __init__(self, width, height, start):\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "    self.i = start[0]\n",
    "    self.j = start[1]\n",
    "\n",
    "  def set(self, rewards, actions):\n",
    "    # rewards should be a dict of: (i, j): r (row, col): reward\n",
    "    # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "    self.rewards = rewards\n",
    "    self.actions = actions\n",
    "\n",
    "  def set_state(self, s):\n",
    "    self.i = s[0]\n",
    "    self.j = s[1]\n",
    "\n",
    "  def current_state(self):\n",
    "    return (self.i, self.j)\n",
    "\n",
    "  def is_terminal(self, s):\n",
    "    return s not in self.actions\n",
    "\n",
    "  def move(self, action):\n",
    "    # check if legal move first\n",
    "    if action in self.actions[(self.i, self.j)]:\n",
    "      same_state = (self.i,self.j)\n",
    "      if action == 'U':\n",
    "        self.i -= 1\n",
    "      elif action == 'D':\n",
    "        self.i += 1\n",
    "      elif action == 'R':\n",
    "        self.j += 1\n",
    "      elif action == 'L':\n",
    "        self.j -= 1\n",
    "    # # if agent hit the wall go back to same position.\n",
    "    if (self.i==4 or self.i==-1):\n",
    "      (self.i,self.j)=same_state\n",
    "    if (self.j==4 or self.j==-1):\n",
    "      (self.i,self.j)=same_state\n",
    "        # return a reward (if any)\n",
    "    return self.rewards.get((self.i, self.j), 0)\n",
    "\n",
    "  def game_over(self):\n",
    "    # returns true if game is over, else false\n",
    "    # true if we are in end state (0,0) or (3,3)\n",
    "    end_state = [(3,3)]\n",
    "    return (self.i, self.j) in end_state\n",
    "\n",
    "  def all_states(self):\n",
    "    # either a position that has possible next actions\n",
    "    # or a position that yields a reward\n",
    "    return set(self.actions.keys()) | set(self.rewards.keys())\n",
    "\n",
    "\n",
    "def standard_grid(starting_position,k1,k2,x1,x2):\n",
    "  # define a grid that describes the reward for arriving at each state\n",
    "  # and possible actions at each state\n",
    "  g = Grid(4, 4, starting_position)\n",
    "  rewards = {\n",
    "    (3, 3): 0,\n",
    "    (k1,k2):-5,\n",
    "    (x1,x2):-5,\n",
    "  }\n",
    "  actions = {\n",
    "    (0, 0): ('L', 'D', 'R','U'),\n",
    "    (0, 1): ('L', 'D', 'R','U'),\n",
    "    (0, 2): ('L', 'D', 'R','U'),\n",
    "    (0, 3): ('L', 'D', 'R','U'),\n",
    "    (1, 0): ('L', 'D', 'R','U'),\n",
    "    (1, 1): ('L', 'D', 'R','U'),\n",
    "    (1, 2): ('L', 'D', 'R','U'),\n",
    "    (1, 3): ('L', 'D', 'R','U'),\n",
    "    (2, 0): ('L', 'D', 'R','U'),\n",
    "    (2, 1): ('L', 'D', 'R','U'),\n",
    "    (2, 2): ('L', 'D', 'R','U'),\n",
    "    (2, 3): ('L', 'D', 'R','U'),\n",
    "    (3, 0): ('L', 'D', 'R','U'),\n",
    "    (3, 1): ('L', 'D', 'R','U'),\n",
    "    (3, 2): ('L', 'D', 'R','U'),\n",
    "    (3, 3): ('L', 'D', 'R','U'),\n",
    "  }\n",
    "  g.set(rewards, actions)\n",
    "  return g\n",
    "\n",
    "\n",
    "def negative_grid(step_cost,k1,k2,x1,x2):\n",
    "  # we minimize the number of moves\n",
    "  # so we will penalize every move (-1) except for the end-state (0)\n",
    "  \n",
    "  states = [[i, j] for i in range(4) for j in range(4)]\n",
    "  initState = random.choice(states[1:-1])\n",
    "  g = standard_grid(initState,k1,k2,x1,x2)\n",
    "  g.rewards.update({\n",
    "    (0, 0): step_cost,\n",
    "    (0, 1): step_cost,\n",
    "    (0, 2): step_cost,\n",
    "    (0, 3): step_cost,\n",
    "    (1, 0): step_cost,\n",
    "    (1, 0): step_cost,\n",
    "    (1, 1): step_cost,\n",
    "    (1, 2): step_cost,\n",
    "    (1, 3): step_cost,\n",
    "    (2, 0): step_cost,\n",
    "    (2, 1): step_cost,\n",
    "    (2, 2): step_cost,\n",
    "    (2, 3): step_cost,\n",
    "    (3, 0): step_cost,\n",
    "    (3, 1): step_cost,\n",
    "    (3, 2): step_cost,\n",
    "    (3, 3): 0,\n",
    "    (k1,k2): -5,\n",
    "    (x1,x2): -5,\n",
    "\n",
    "  })\n",
    "  return g\n",
    "\n",
    "\n",
    "def print_values(V, g):\n",
    "  for i in range(g.width):\n",
    "    print(\"---------------------------\")\n",
    "    for j in range(g.height):\n",
    "      v = V.get((i,j), 0)\n",
    "      if v >= 0:\n",
    "        print(\" %.2f|\" % v, end=\"\")\n",
    "      else:\n",
    "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "def print_policy(P, g):\n",
    "  for i in range(g.width):\n",
    "    print(\"---------------------------\")\n",
    "    for j in range(g.height):\n",
    "      a = P.get((i,j), ' ')\n",
    "      print(\"  %s  |\" % a, end=\"\")\n",
    "    print(\"\")\n",
    "# the Monte Carlo Epsilon-Greedy method to find the optimal policy and value function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "GAMMA = 0.7\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "def random_action(a, eps=0.1):\n",
    "  # choose given a with probability 1 - eps + eps/4\n",
    "  p = np.random.random()\n",
    "  if p < (1 - eps):\n",
    "    return a\n",
    "  else:\n",
    "    return np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "def max_dict(d):\n",
    "  # returns the argmax (key) and max (value) from a dictionary\n",
    "  max_key = None\n",
    "  max_val = float('-inf')\n",
    "  for k, v in d.items():\n",
    "    if v > max_val:\n",
    "      max_val = v\n",
    "      max_key = k\n",
    "  return max_key, max_val\n",
    "\n",
    "def play_game(grid, policy):\n",
    "  # returns a list of states and corresponding returns\n",
    "  # use an epsilon-soft policy\n",
    "  # Random start position\n",
    "  states = [(i, j) for i in range(4) for j in range(4)]\n",
    "  initState = random.choice(states[1:-1])\n",
    "  g = standard_grid(initState,k1,k2,x1,x2)\n",
    "  s = initState\n",
    "  grid.set_state(s)\n",
    "  a = random_action(policy[s])\n",
    "\n",
    "  # each triple is s(t), a(t), r(t)\n",
    "  # but r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
    "  states_actions_rewards = [(s, a, 0)]\n",
    "  while True:\n",
    "    r = grid.move(a)\n",
    "    s = grid.current_state()\n",
    "    if grid.game_over():\n",
    "      states_actions_rewards.append((s, None, r))\n",
    "      break\n",
    "    else:\n",
    "      a = random_action(policy[s]) # the next state is stochastic\n",
    "      states_actions_rewards.append((s, a, r))\n",
    "\n",
    "  # calculate the returns by working backwards from the terminal state\n",
    "  G = 0\n",
    "  states_actions_returns = []\n",
    "  first = True\n",
    "  for s, a, r in reversed(states_actions_rewards):\n",
    "    # the value of the terminal state is 0 \n",
    "    # we should ignore the first state we encounter\n",
    "    # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "    if first:\n",
    "      first = False\n",
    "    else:\n",
    "      states_actions_returns.append((s, a, G))\n",
    "    G = r + GAMMA*G\n",
    "  states_actions_returns.reverse() # we want it to be in order of state visited\n",
    "  return states_actions_returns\n",
    "\n",
    "for i in range(0,4):\n",
    "  master_policy[i]={}\n",
    "  for i1 in range(0,4):\n",
    "    if (i==3 & i1==3):\n",
    "      k1=2\n",
    "      k2=2\n",
    "    else:\n",
    "      k1=i\n",
    "      k2=i1\n",
    "    master_policy[i][i1]={}\n",
    "    for s0 in range(0,4):\n",
    "      master_policy[i][i1][s0]={}\n",
    "      for s1 in range(0,4): \n",
    "        if (s0==3 & s1==3):\n",
    "          x1=2\n",
    "          x2=2\n",
    "        else:\n",
    "          x1=s0\n",
    "          x2=s1\n",
    "        grid = negative_grid(-1,k1,k2,x1,x2)\n",
    "# print rewards\n",
    "        print(\"rewards:\")\n",
    "        print_values(grid.rewards, grid)\n",
    "\n",
    "# state -> action\n",
    "# initialize a random policy\n",
    "        policy = {}\n",
    "        for s in grid.actions.keys():\n",
    "          policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "  \n",
    "# initial policy\n",
    "        print(\"initial policy:\")\n",
    "        print_policy(policy, grid)\n",
    "\n",
    "# initialize Q(s,a) and returns\n",
    "        Q = {}\n",
    "        returns = {} # dictionary of state -> list of returns we've received\n",
    "        states = grid.all_states()\n",
    "        for s in states:\n",
    "          if s in grid.actions: # not a terminal state\n",
    "            Q[s] = {}\n",
    "            for a in ALL_POSSIBLE_ACTIONS:\n",
    "              Q[s][a] = 0\n",
    "              returns[(s,a)] = []\n",
    "          else:\n",
    "    # terminal state or state we can't otherwise get to\n",
    "            pass\n",
    "  \n",
    "# initial Q values for all states in grid\n",
    "        print(Q)\n",
    "        print(s)\n",
    "\n",
    "\n",
    "\n",
    "# repeat\n",
    "        deltas = []\n",
    "        for t in range(1000):\n",
    "  # generate an episode using pi\n",
    "          biggest_change = 0\n",
    "          states_actions_returns = play_game(grid, policy)\n",
    "\n",
    "  # calculate Q(s,a)\n",
    "          seen_state_action_pairs = set()\n",
    "          for s, a, G in states_actions_returns:\n",
    "    # check if we have already seen s\n",
    "    # called \"first-visit\" MC policy evaluation\n",
    "            sa = (s, a)\n",
    "            if sa not in seen_state_action_pairs:\n",
    "              old_q = Q[s][a]\n",
    "              returns[sa].append(G)\n",
    "              Q[s][a] = np.mean(returns[sa])\n",
    "              biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
    "              seen_state_action_pairs.add(sa)\n",
    "          deltas.append(biggest_change)\n",
    "\n",
    "  # calculate new policy pi(s) = argmax[a]{ Q(s,a) }\n",
    "          for s in policy.keys():\n",
    "            a, _ = max_dict(Q[s])\n",
    "            policy[s] = a\n",
    "\n",
    "\n",
    "        plt.plot(deltas)\n",
    "        plt.show()\n",
    "\n",
    "# find the optimal state-value function\n",
    "# V(s) = max[a]{ Q(s,a) }\n",
    "        V = {}\n",
    "        for s in policy.keys():\n",
    "          V[s] = max_dict(Q[s])[1]\n",
    "\n",
    "# Print the table of the estimated function Q(s,a) for the optimal policy \n",
    "        print(\"final values:\")\n",
    "        print_values(V, grid)\n",
    "\n",
    "        print(\"final policy:\")\n",
    "# Print the policy pi(s) \n",
    "        print_policy(policy, grid)\n",
    "        print(policy)\n",
    "        print(\"#########################\")\n",
    "        print(i,i1,s0,s1)\n",
    "        master_policy[i][i1][s0][s1]={}\n",
    "        master_policy[i][i1][s0][s1]=policy\n",
    "        print(\"#########################\")\n",
    "       \n",
    "      \n",
    "\n",
    "print (master_policy)\n",
    "\n",
    "full_obstacle_list = [(10,200),(250, 110), (200,280),(220,200), (300,300), (200,100), (400,150),(400, 110), \n",
    "                 (430, 300), (201, 304), (135, 281), \n",
    "                 (286, 373), (175, 280), (250, 375), (139, 327), (369, 278), \n",
    "                 (295, 196),(40,45),(60, 100),(210, 111),(100,100),(110,110),(79,250)]\n",
    "pos=(430, 390)\n",
    "print(check_obstacle(pos,full_obstacle_list))\n",
    "\n",
    "def check_obstacle(obs, obs_list):\n",
    "  location_on_map,location_on_grid = find_location_onMap(obs)\n",
    "  print(location_on_map)\n",
    "  for i in full_obstacle_list:\n",
    "    location_on_map1,location_on_grid1 = find_location_onMap(i)\n",
    "    if (location_on_map1 == location_on_map):\n",
    "      print(i)\n",
    "      print(location_on_map1)\n",
    "      return True\n",
    "  return False\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Playing_Phase'></a>\n",
    "## Playing Phase\n",
    "\n",
    "With the Master Policy trained, we adapted the original code of the Robot World to our create our New Robot World. We included the Master Policy as a dictionary considering States and Actions. We also created to functions: find_location_onMap and policy_finder, which are use to get information of the robot and obstacles inside an specific grid and define the optimal actions according the these positions. \n",
    "\n",
    "Inside the code where the original program understand what kind of actions the robot should take, we change the code to call our policy_finder fuction, which discovers the robot and obstacle locations, and return the action that the robot must take, according our Master Policy. \n",
    "\n",
    "Now you can play with the New Robot World clicking on the step button and adding new obstacles. \n",
    "\n",
    "Here is the link to the New Robot World:\n",
    "\n",
    "http://www.codeskulptor.org/#user47_JJSgNIMtcq_0.py\n",
    "\n",
    "Following is the modified code that change the original simulator to the New Robot World."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert below the code that plays with the robot to avoid obstacles with RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simplegui\n",
    "import math\n",
    "import random\n",
    "\n",
    "#define constants\n",
    "\n",
    "\n",
    "# New Code\n",
    "\n",
    "master_policy={0: {0: {(0, 0): 'R', (0, 1): 'R', (0, 2): 'R', (0, 3): 'D', (1, 0): 'D', (1, 1): 'L', (1, 2): 'U', (1, 3): 'D', (2, 0): 'R', (2, 1): 'D', (2, 2): 'D', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}, 1: {(0, 0): 'R', (0, 1): 'R', (0, 2): 'R', (0, 3): 'D', (1, 0): 'D', (1, 1): 'R', (1, 2): 'R', (1, 3): 'D', (2, 0): 'D', (2, 1): 'L', (2, 2): 'R', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}, 2: {(0, 0): 'R', (0, 1): 'R', (0, 2): 'D', (0, 3): 'D', (1, 0): 'U', (1, 1): 'R', (1, 2): 'R', (1, 3): 'D', (2, 0): 'R', (2, 1): 'D', (2, 2): 'R', (2, 3): 'D', (3, 0): 'U', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}, 3: {(0, 0): 'R', (0, 1): 'R', (0, 2): 'R', (0, 3): 'D', (1, 0): 'U', (1, 1): 'R', (1, 2): 'R', (1, 3): 'D', (2, 0): 'U', (2, 1): 'D', (2, 2): 'R', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}}, 1: {0: {(0, 0): 'R', (0, 1): 'R', (0, 2): 'R', (0, 3): 'D', (1, 0): 'R', (1, 1): 'D', (1, 2): 'R', (1, 3): 'D', (2, 0): 'D', (2, 1): 'D', (2, 2): 'D', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}, 1: {(0, 0): 'D', (0, 1): 'L', (0, 2): 'R', (0, 3): 'D', (1, 0): 'D', (1, 1): 'D', (1, 2): 'D', (1, 3): 'D', (2, 0): 'D', (2, 1): 'D', (2, 2): 'D', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}, 2: {(0, 0): 'D', (0, 1): 'L', (0, 2): 'L', (0, 3): 'D', (1, 0): 'D', (1, 1): 'D', (1, 2): 'R', (1, 3): 'D', (2, 0): 'R', (2, 1): 'D', (2, 2): 'L', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}, 3: {(0, 0): 'R', (0, 1): 'D', (0, 2): 'D', (0, 3): 'L', (1, 0): 'D', (1, 1): 'D', (1, 2): 'D', (1, 3): 'L', (2, 0): 'D', (2, 1): 'R', (2, 2): 'R', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}}, 2: {0: {(0, 0): 'R', (0, 1): 'R', (0, 2): 'D', (0, 3): 'D', (1, 0): 'R', (1, 1): 'R', (1, 2): 'D', (1, 3): 'D', (2, 0): 'D', (2, 1): 'R', (2, 2): 'R', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}, 1: {(0, 0): 'R', (0, 1): 'R', (0, 2): 'R', (0, 3): 'D', (1, 0): 'U', (1, 1): 'R', (1, 2): 'D', (1, 3): 'D', (2, 0): 'D', (2, 1): 'L', (2, 2): 'R', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}, 2: {(0, 0): 'R', (0, 1): 'R', (0, 2): 'R', (0, 3): 'D', (1, 0): 'D', (1, 1): 'U', (1, 2): 'R', (1, 3): 'D', (2, 0): 'D', (2, 1): 'D', (2, 2): 'R', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}, 3: {(0, 0): 'R', (0, 1): 'D', (0, 2): 'D', (0, 3): 'L', (1, 0): 'U', (1, 1): 'D', (1, 2): 'D', (1, 3): 'L', (2, 0): 'U', (2, 1): 'R', (2, 2): 'D', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}}, 3: {0: {(0, 0): 'R', (0, 1): 'R', (0, 2): 'D', (0, 3): 'D', (1, 0): 'U', (1, 1): 'U', (1, 2): 'D', (1, 3): 'D', (2, 0): 'U', (2, 1): 'D', (2, 2): 'R', (2, 3): 'D', (3, 0): 'R', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}, 1: {(0, 0): 'R', (0, 1): 'R', (0, 2): 'R', (0, 3): 'D', (1, 0): 'U', (1, 1): 'D', (1, 2): 'R', (1, 3): 'D', (2, 0): 'R', (2, 1): 'R', (2, 2): 'D', (2, 3): 'D', (3, 0): 'U', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}, 2: {(0, 0): 'R', (0, 1): 'R', (0, 2): 'D', (0, 3): 'D', (1, 0): 'R', (1, 1): 'R', (1, 2): 'R', (1, 3): 'D', (2, 0): 'U', (2, 1): 'U', (2, 2): 'R', (2, 3): 'D', (3, 0): 'R', (3, 1): 'U', (3, 2): 'R', (3, 3): 'U'}, 3: {(0, 0): 'D', (0, 1): 'D', (0, 2): 'R', (0, 3): 'D', (1, 0): 'R', (1, 1): 'R', (1, 2): 'R', (1, 3): 'D', (2, 0): 'U', (2, 1): 'D', (2, 2): 'D', (2, 3): 'D', (3, 0): 'U', (3, 1): 'R', (3, 2): 'R', (3, 3): 'U'}}}\n",
    "\n",
    "# New Code\n",
    "\n",
    "\n",
    "OBSTACLE_RAD = 20 # how big (radius) are the obstacles\n",
    "ROBOT_RAD = 30 # how big (radius) is the robot?\n",
    "SENSOR_FOV = 10.0 # FOV of each sensor THIS MUST BE A FLOAT!!!!!! \n",
    "SENSOR_MAX_R = 100 # max range that each sensor can report\n",
    "SENSOR_ALERT_R = 50 #range within which sensor reports are acted upon\n",
    "TURN_SCALE_FACTOR = 2 # how drastic do we want the turns to be\n",
    "SAFETY_DISTANCE = 40\n",
    "#helper functions\n",
    "\n",
    "\n",
    "# New Code\n",
    "\n",
    "\n",
    "def find_location_onMap(pos):\n",
    "  location_in_the_grid=[]\n",
    "  location_in_the_map=[]\n",
    "  i=0\n",
    "  i1=0\n",
    "  loc=0\n",
    "  loc1=0\n",
    "  for squares in range(0,10):\n",
    "    i1=0\n",
    "    for quare in range(0,10):\n",
    "      loc=0\n",
    "      #square.append((i,i1))\n",
    "      if ((pos[0]>i and pos[0]<=i+50) and (pos[1]>i1 and pos[1]<=i1+50)):\n",
    "        for rows in range(0,4):\n",
    "          loc1=0\n",
    "          for col in range(0,4):\n",
    "            if ((pos[0]>loc+i and pos[0]<=loc+i+12.5) and (pos[1]>loc1+i1 and pos[1]<=loc1+12.5+i1)):           \n",
    "              location_in_the_grid.append(loc)\n",
    "              location_in_the_grid.append(loc1)\n",
    "              location_in_the_map.append(i)\n",
    "              location_in_the_map.append(i1)\n",
    "            \n",
    "            loc1+=12.5\n",
    "          loc+=12.5\n",
    "      i1+=50\n",
    "    i+=50\n",
    "\n",
    "#To to convert to 3x3 grid\n",
    "  location_in_the_grid[0]=int(location_in_the_grid[0]/12.5)\n",
    "  location_in_the_grid[1]=int(location_in_the_grid[1]/12.5)\n",
    "#To to convert to 5x5 map each sqaure is 100X100 pixel\n",
    "  location_in_the_map[0]=int(location_in_the_map[0]/50)\n",
    "  location_in_the_map[1]=int(location_in_the_map[1]/50)\n",
    "  return location_in_the_map, location_in_the_grid\n",
    "\n",
    "\n",
    "def policy_finder (mylocation,obs):\n",
    "  mylocation_onMap, my_location_onGrid = find_location_onMap(mylocation)\n",
    "  obs_location_onMap, obs_location_onGrid = find_location_onMap(obs)\n",
    "#  print(\"My location: \", mylocation_onMap)\n",
    "#  print(\"Obstacle location: \", obs_location_onMap)\n",
    "  if (mylocation_onMap == obs_location_onMap):\n",
    "    print(\"Match found\")\n",
    "    policy= master_policy[obs_location_onGrid[0]][obs_location_onGrid[1]]\n",
    "    direction = policy.get((my_location_onGrid[0],my_location_onGrid[1]), ' ')\n",
    "    print(direction)\n",
    "    return direction\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "# New Code\n",
    "\n",
    "\n",
    "def rel_brg_fm_offset_sensor(true_hdg, sensor_offset, tgt_brg):\n",
    "    #given robot's true heading, the sensor offset angle and the\n",
    "    #true brg of the target, this fn will return the relative brg\n",
    "    #of the target from the sensor's line of sight\n",
    "    \n",
    "    sensor_look_brg = (true_hdg + sensor_offset)%360\n",
    "    tgt_rel_fm_sensor = tgt_brg - sensor_look_brg\n",
    "\n",
    "    if tgt_rel_fm_sensor < -180:\n",
    "        tgt_rel_fm_sensor += 360\n",
    "    \n",
    "    return tgt_rel_fm_sensor\n",
    "\n",
    "def brg_in_deg(p0, p1):#bearing only in degrees\n",
    "    [x1, y1] = p0\n",
    "    [x2, y2] = p1\n",
    "    a = math.degrees(math.atan((y1 - y2)/(x1 - x2 + 0.000000001)))\n",
    "    #find and correct the quadrant...\n",
    "    if  x2 >= x1:\n",
    "        b = 90 + a\n",
    "    else:\n",
    "        b = 270 + a\n",
    "    return b\n",
    "\n",
    "def dist(p1, p0):#distance only\n",
    "    return math.sqrt((p1[0] - p0[0])**2+(p1[1]-p0[1])**2)\n",
    "\n",
    "def dist_and_brg_in_deg(p0, p1):#bearing and distance in degrees between two points\n",
    "    [x1, y1] = p0\n",
    "    [x2, y2] = p1\n",
    "    r = math.sqrt((x1 - x2)**2 + (y1 - y2)**2) # distance\n",
    "    a = math.degrees(math.atan((y1 - y2)/(x1 - x2 + 0.000000001)))\n",
    "    #find and correct the quadrant...\n",
    "    if  x2 >= x1:\n",
    "        b = 90 + a\n",
    "    else:\n",
    "        b = 270 + a\n",
    "    return r, b\n",
    "\n",
    "def angle_to_vector(ang):#resolve angles into vectors\n",
    "    ang = math.radians(ang)\n",
    "    return [math.cos(ang), math.sin(ang)]\n",
    "\n",
    "def relative_brg(b1, b2):\n",
    "    rb = b2 - b1\n",
    "    if rb > 180:\n",
    "        rb = 360 - rb\n",
    "    if rb < -180:\n",
    "        rb += 360\n",
    "        rb *= -1\n",
    "    return rb\n",
    "\n",
    "def create_vector(from_pos, length, brg):       \n",
    "        u_vec = angle_to_vector(brg)\n",
    "        #print \"generating target line...\"\n",
    "        vec0 = from_pos[0] + length * u_vec[1] \n",
    "        vec1 = from_pos[1] - length * u_vec[0]\n",
    "        \n",
    "        return [vec0, vec1]\n",
    "\n",
    "    \n",
    "#define classes\n",
    "\n",
    "class Sonar:\n",
    "    def __init__(self, index, FOV, max_r, robot_co):\n",
    "        #create a instance of this class\n",
    "        self.pos = [0,0]\n",
    "        self.index = index\n",
    "        self.max_r = max_r\n",
    "        self.offset =  index * FOV #+ FOV/2 # on what relative bearing is this sensor looking?\n",
    "        self.look_brg = (robot_co + self.offset)%360\n",
    "        self.vec = [0,0] # just a vector for grpahical ouptut of pings\n",
    "        self.has_valid_echo = False #indicates if this sonar has a \"valid\" obstacle in sight\n",
    "        #print \"Creating Sonar:\" , index, \" offset \", self.offset, \"true LOS:\", self.look_brg\n",
    "            \n",
    "    def ping_actual():#ping for real in a robot and return range observed\n",
    "        pass\n",
    "    \n",
    "    def ping_simulated(self, obstacle_list, robot_co):\n",
    "        #from robot position and robot_co, run through obs_list\n",
    "        #return the distance to closest object within \n",
    "        #FOV and within self.max_r of THIS SENSOR\n",
    "        # range all the obstacles in view, find the nearest\n",
    "        \n",
    "        range_list = []\n",
    "        \n",
    "        for obs in obstacle_list:# find objects within max_r and inside FOV\n",
    "            #print \"pinging for robot_co:\", robot_co\n",
    "            can_observe, d = self.can_observe(robot_co, obs, OBSTACLE_RAD)\n",
    "            #print can_observe, d\n",
    "            if can_observe:\n",
    "                range_list.append(d) \n",
    "        if len(range_list) > 0:\n",
    "            self.output = min(range_list)- SAFETY_DISTANCE\n",
    "        else:\n",
    "            self.output = SENSOR_MAX_R\n",
    "        #print \"closest point: \" , obs , \" distance:\", self.output\n",
    "    \n",
    "    def get_output(self):\n",
    "        return self.output\n",
    "    \n",
    "    def can_observe(self, robot_co, obstacle_pos, obstacle_rad):\n",
    "        #if obstacle is within within the max_r of the sensor\n",
    "        #and within FOV, return True and the distance observed\n",
    "        #else return False, 0\n",
    "        \n",
    "        dist, brg = dist_and_brg_in_deg(self.pos, obstacle_pos)\n",
    "        \n",
    "        if dist < self.max_r: #if the object is within max_r....\n",
    "            rel_brg = rel_brg_fm_offset_sensor(robot_co, self.offset, brg)#rel brg of tgt from sensor LOS\n",
    "            d_test = abs(dist * math.asin(math.radians(rel_brg)))\n",
    "            if d_test < OBSTACLE_RAD + ROBOT_RAD: \n",
    "                self.has_valid_echo = True\n",
    "                return True, dist # if the object is within min allowed lateral separation\n",
    "            else:\n",
    "                self.has_valid_echo = False\n",
    "                return False, 0 # ignore it\n",
    "        else:#if the object is outside max_r of this sonar...ignore it\n",
    "            self.has_valid_echo = False\n",
    "            return False, 0\n",
    "      \n",
    "    def update(self, platform_pos, platform_co, obstacle_list):\n",
    "              \n",
    "        #update own parameters\n",
    "        self.pos = platform_pos\n",
    "        \n",
    "        self.look_brg = (platform_co + self.offset)%360\n",
    "        \n",
    "        #calculate output of this sensor\n",
    "        \n",
    "        self.ping_simulated(obstacle_list, platform_co)\n",
    "        \n",
    "        self.vec = create_vector(self.pos, self.output + ROBOT_RAD, self.look_brg)#calculate distance vector for drawing on canvas\n",
    "        \n",
    "        #print \"sensor index:\", self.index, \" look brg:\", self.look_brg\n",
    "    def draw(self, canvas): # draw the sensor's output\n",
    "        #if self.has_valid_echo:\n",
    "        canvas.draw_line(self.pos,self.vec, 1, 'lime')\n",
    "        canvas.draw_text(str(self.index),(self.vec[0]+4, self.vec[1]+4), 10, \"lime\"),\n",
    "        #print self.index, \" VE:\" , self.has_valid_echo\n",
    "        \n",
    "class Sonar_Array:\n",
    "    def __init__(self, n_sensors, SENSOR_FOV, SENSOR_MAX_R, robot_co):\n",
    "        self.sonar_list = []\n",
    "        self.need_diversion_flag = False\n",
    "        \n",
    "        i_pos = range(1, n_sensor/2 + 1)\n",
    "        i_neg = range(-n_sensor/2 , 0)\n",
    "        i_pos.reverse()\n",
    "        i_neg.reverse()\n",
    "        i_pos.extend(i_neg)\n",
    "        \n",
    "        for i in i_pos:#create a list of individual sonars...\n",
    "            self.sonar_list.append(Sonar(i, SENSOR_FOV, SENSOR_MAX_R, robot_co))\n",
    "   \n",
    "    def update(self, robot_pos, robot_co, obstacle_list, method):\n",
    "        #update sonar array\n",
    "        for sonar in self.sonar_list:#update output of each sensor\n",
    "            sonar.update(robot_pos, robot_co, obstacle_list)\n",
    "            \n",
    "        if method == \"w_sum\":#process data by method of weighted sums\n",
    "            return self.weighted_sum_method(robot_pos, robot_co)\n",
    "    \n",
    "    def weighted_sum_method(self, robot_pos, robot_co):\n",
    "        #process data by the weighted sum method and \n",
    "        #return (1) whether turn is required or not (2) index of recommended sonar LOS to turn to\n",
    "        sum_d = 0\n",
    "        sum_wt = 0\n",
    "        alert = False\n",
    "        #print \"checking all sonars:\"      \n",
    "        for sonar in self.sonar_list:\n",
    "            #print \"sonar:\", sonar.index, \" range:\", sonar.output\n",
    "            if sonar.output < SENSOR_ALERT_R:#has this sonar found anything in danger zone?\n",
    "                alert = True\n",
    "                #print \"obstacle found by index \", sonar.index\n",
    "                for s1 in self.sonar_list: #process the whole array\n",
    "                    d = int(s1.output)\n",
    "                    gain = 1#SENSOR_MAX_R/(SENSOR_MAX_R - d)\n",
    "                    sum_d +=  d\n",
    "                    sum_wt += s1.index * d * gain\n",
    "                    #print \"I:\", s1.index,\",D:\",int(s1.output), \",sum_D:\", sum_d, \"sum_wt:\",sum_wt\n",
    "                rec_index = math.ceil(TURN_SCALE_FACTOR * float(sum_wt)/sum_d) #index of sonar with best LOS\n",
    "                #rec_index = int(TURN_SCALE_FACTOR * float(sum_d)/sum_wt)\n",
    "                print \"Rec index:\", rec_index\n",
    "                if abs(rec_index) > n_sensor/2:\n",
    "                    print \"rec index too large\"\n",
    "                    rec_index = n_sensor/2\n",
    "#                    if rec_index < 0:\n",
    "#                        rec_index = -n_sensor/2\n",
    "#                    else:\n",
    "#                        rec_index = n_sensor/2\n",
    "                print \"Rec index:\", rec_index \n",
    "                break # processing completed\n",
    "            else: #no obstacle in danger zone\n",
    "                rec_index = 0\n",
    "                #return robot_co, False\n",
    "                #print : index = \", sonar.index\n",
    "        #print \"break from loop.\"\n",
    "        if rec_index == 0 and alert == True:\n",
    "            print \"alert with no alteration\"\n",
    "            \n",
    "# New Code\n",
    "            \n",
    "            for obs in full_obstacle_list:\n",
    "                    mylocation=robot_pos\n",
    "                    mylocation_onMap, my_location_onGrid = find_location_onMap(mylocation)\n",
    "                    obs_location_onMap, obs_location_onGrid = find_location_onMap(obs)\n",
    "                    action=None\n",
    "                    if (mylocation_onMap == obs_location_onMap):\n",
    "                        action = policy_finder(mylocation,obs)\n",
    "                        if action != None:\n",
    "                            if action == 'R':\n",
    "                                offset=90\n",
    "                            elif action == 'D':\n",
    "                                offset=180\n",
    "                            elif action == 'L':\n",
    "                                offset=270\n",
    "                            elif action == 'U':\n",
    "                                offset = 359\n",
    "                        break\n",
    "            if (action !=None):\n",
    "                print(robot_pos)\n",
    "                print(\"*********\")\n",
    "                print((offset%robot_co)+robot_co)\n",
    "                print(\"******\")\n",
    "                return (offset%robot_co)+robot_co, True \n",
    "            else:\n",
    "               return robot_co, True    \n",
    "        \n",
    "# New Code\n",
    "        \n",
    "                \n",
    "        elif abs(rec_index) > 0: # some alteration recommended\n",
    "            print \"turn recommended\"\n",
    "            offset =  rec_index * SENSOR_FOV #how much is the angular offset  \n",
    "            \n",
    "# New Code \n",
    "            \n",
    "            for obs in full_obstacle_list:\n",
    "                mylocation=robot_pos\n",
    "                mylocation_onMap, my_location_onGrid = find_location_onMap(mylocation)\n",
    "                obs_location_onMap, obs_location_onGrid = find_location_onMap(obs)\n",
    "                action=None\n",
    "                if (mylocation_onMap == obs_location_onMap):\n",
    "                    action = policy_finder(mylocation,obs)\n",
    "                    if action != None:\n",
    "                        if action == 'R':\n",
    "                            offset=90\n",
    "                        elif action == 'D':\n",
    "                            offset=180\n",
    "                        elif action == 'L':\n",
    "                            offset=270\n",
    "                        elif action == 'U':\n",
    "                            offset = 359\n",
    "                    break\n",
    "            print(robot_pos)\n",
    "            print(\"*********\")\n",
    "            print(action)\n",
    "            print((offset%robot_co)+robot_co)\n",
    "            print(\"******\")\n",
    "            if (action==None):\n",
    "               return robot_co, False \n",
    "            return (offset%robot_co), True\n",
    "        \n",
    "# New Code        \n",
    "        \n",
    "        else:# no diversion needed\n",
    "            print \"no alert no diversion\"\n",
    "            return robot_co, False\n",
    "    \n",
    "    def draw(self, canvas):\n",
    "        for sonar in self.sonar_list:\n",
    "            sonar.draw(canvas)\n",
    "\n",
    "class Robot:\n",
    "    def __init__(self, pos, co, n_sensor):\n",
    "        self.pos = pos\n",
    "        self.history = [pos]\n",
    "        self.co = co\n",
    "        self.spd = 10 # robot speed in pixels/ step\n",
    "        self.s_array = Sonar_Array(n_sensor, SENSOR_FOV, SENSOR_MAX_R, self.co)\n",
    "        self.goal_brg = brg_in_deg(self.pos, goal_pos)\n",
    "        self.obstacles_in_view = []\n",
    "    \n",
    "    def get_obstacles_in_view(self):\n",
    "        return self.obstacles_in_view\n",
    "    \n",
    "    def update(self):\n",
    "        self.obstacles_in_view = [] #delete all the old obstacles in view\n",
    "        for obs in full_obstacle_list:\n",
    "            if dist(self.pos, obs) < SENSOR_MAX_R:\n",
    "                self.obstacles_in_view.append(obs)\n",
    "                \n",
    "        #re-calculate direction to goal\n",
    "        self.goal_brg = brg_in_deg(self.pos, goal_pos)\n",
    "        #re-estimate sensor output by weighted sum method\n",
    "        co1, need_turn = self.s_array.update(self.pos, self.goal_brg, self.obstacles_in_view, \"w_sum\")\n",
    "        #print \"Path Clear:\", self.path_is_clear()\n",
    "        if self.path_is_clear():#can we reach the goal directly from here?\n",
    "            self.co = brg_in_deg(self.pos, goal_pos)\n",
    "            print \"path clear. ignoring recommendation\"\n",
    "        elif need_turn: #do we need to turn\n",
    "            self.co = co1\n",
    "            print \"path not clear. following recommendation\"\n",
    "        else: # path is not fully clear, but there are no immediate obstacles\n",
    "            pass\n",
    "            #self.co = brg_in_deg(self.pos, goal_pos)\n",
    "\n",
    "        #m5e the robot by one step...\n",
    "        self.move(2)\n",
    "\n",
    "    def path_is_clear(self):#return True if there is a clear path to the goal\n",
    "        goal_brg = brg_in_deg(self.pos, goal_pos)\n",
    "        for obs in self.obstacles_in_view:\n",
    "            if dist(self.pos, goal_pos) > dist(self.pos, obs):\n",
    "                d_obs, obs_brg = dist_and_brg_in_deg(self.pos, obs)\n",
    "                rel_brg = abs(relative_brg(goal_brg, obs_brg))\n",
    "                d_lateral = abs(d_obs * math.asin(math.radians(rel_brg)))\n",
    "                if d_lateral < OBSTACLE_RAD + ROBOT_RAD: \n",
    "                    return False\n",
    "        return True\n",
    "    \n",
    "    def move(self, dT):\n",
    "        print(\"VVVVVBVV\")        \n",
    "        print(self.co)\n",
    "        u_vec = angle_to_vector(self.co)\n",
    "        \n",
    "        self.pos[0] += self.spd * dT * u_vec[1]\n",
    "        self.pos[1] -= self.spd * dT * u_vec[0]\n",
    "        \n",
    "        self.history.append([self.pos[0], self.pos[1]])\n",
    "        \n",
    "    def get_pos(self):\n",
    "        return self.pos\n",
    "    \n",
    "    def set_pos(self, pos):\n",
    "        self.pos = pos\n",
    "    \n",
    "    def set_co(self, co):\n",
    "        self.co = co\n",
    "        #print \"setting robot co:\", self.co\n",
    "    \n",
    "    def delete_history(self):\n",
    "        self.history = []\n",
    "   \n",
    "    def draw(self, canvas):\n",
    "        #Draw the robot\n",
    "        canvas.draw_circle(self.pos, 4, 3, \"yellow\")\n",
    "        canvas.draw_text(\"R\", [self.pos[0] + 10, self.pos[1] +10], 16, \"yellow\")\n",
    "        #Draw brg line to goal\n",
    "        self.goal_vec = create_vector(self.pos, 150, self.goal_brg)\n",
    "        canvas.draw_line(self.pos, self.goal_vec, 2, \"teal\")\n",
    "        #Draw current heading vector\n",
    "        self.co_vec = create_vector(self.pos, 150, self.co)\n",
    "        canvas.draw_line(self.pos, self.co_vec, 2, \"white\")\n",
    "        #draw the output of the sonar array\n",
    "        self.s_array.draw(canvas)\n",
    "        #draw the obstacles in view\n",
    "        for obs in self.obstacles_in_view:\n",
    "            canvas.draw_circle(obs,2,1, \"red\")\n",
    "            canvas.draw_circle(obs,OBSTACLE_RAD, 1, \"green\") \n",
    "        #draw history\n",
    "        for point in self.history:\n",
    "            canvas.draw_circle(point,2,2, \"lime\")\n",
    "        \n",
    "        \n",
    "#define globals\n",
    "\n",
    "g_state = \"None\"\n",
    "\n",
    "start_pos = [10,10]\n",
    "robot_pos = [10, 10]\n",
    "robot_co = 1\n",
    "\n",
    "goal_pos = [500,500]\n",
    "#obstacle_list = [(300, 213), (310, 124), (250, 110), (300, 230)]\n",
    "\n",
    "\n",
    "# New Code\n",
    "\n",
    "\n",
    "full_obstacle_list = [(10,200),(250, 110), (200,280),(220,200), (300,300), (200,100), (400,150),(400, 110), \n",
    "                 (430, 390), (201, 304), (135, 281), \n",
    "                 (286, 373), (175, 280), (250, 375), (139, 327), (369, 278), \n",
    "                 (295, 196), (210, 111),(100,100),(110,110),(79,250)]\n",
    "                 \n",
    "\n",
    "# New Code\n",
    "                 \n",
    "#create a robot with 6 sensors\n",
    "\n",
    "n_sensor = 16 \n",
    "\n",
    "#create a sonar array\n",
    "#s1 = Sonar_Array(n_sensor, SENSOR_FOV, SENSOR_MAX_R, robot_co)\n",
    "r1 = Robot(robot_pos, robot_co, n_sensor)\n",
    "\n",
    "r1.update()\n",
    "#define event handlers\n",
    "\n",
    "def click(pos):\n",
    "    global g_state, start_pos, goal_pos, robot_pos\n",
    "    if g_state == \"Start\":\n",
    "        start_pos = pos\n",
    "        r1.set_pos(list(pos))\n",
    "    elif g_state == \"Goal\":\n",
    "        goal_pos = pos\n",
    "        r1.set_co(brg_in_deg(r1.get_pos(), pos))\n",
    "    elif g_state == \"Set Robot\":\n",
    "        r1.set_co(brg_in_deg(r1.get_pos(), pos))\n",
    "        r1.set_pos(list(pos))\n",
    "        r1.delete_history()\n",
    "    elif g_state == \"Add Obs\":\n",
    "        full_obstacle_list.append(pos)\n",
    "        print full_obstacle_list\n",
    "        #update the robot\n",
    "    r1.update()\n",
    "    g_state = \"None\"\n",
    "        \n",
    "def set_start():\n",
    "    global g_state\n",
    "    g_state = \"Start\"\n",
    "    \n",
    "def set_goal():\n",
    "    global g_state\n",
    "    g_state = \"Goal\"\n",
    "\n",
    "def set_robot_pos():\n",
    "    global g_state\n",
    "    g_state = \"Set Robot\"\n",
    "\n",
    "def alter_co(text):\n",
    "    r1.set_co(float(text))\n",
    "    r1.update()\n",
    "            \n",
    "def draw(canvas):\n",
    "    #draw start \n",
    "    canvas.draw_circle(start_pos, 4, 3, \"red\")\n",
    "    canvas.draw_text(\"S\", [start_pos[0] + 10, start_pos[1] +10], 16, \"red\")\n",
    "    #draw goal\n",
    "    canvas.draw_circle(goal_pos, 4, 3, \"green\")\n",
    "    canvas.draw_text(\"G\", [goal_pos[0] + 10, goal_pos[1] +10], 16, \"green\")\n",
    "    #draw the obstacles\n",
    "    for obs in full_obstacle_list:\n",
    "        canvas.draw_circle(obs,2,1, \"red\")\n",
    "        canvas.draw_circle(obs,OBSTACLE_RAD, 1, \"white\") \n",
    "    \n",
    "    #draw sonar lines...\n",
    "    r1.draw(canvas)\n",
    "\n",
    "def step():\n",
    "        r1.update()\n",
    "\n",
    "def add_obs():\n",
    "    global g_state\n",
    "    g_state = \"Add Obs\"\n",
    "    \n",
    "    \n",
    "#create simplegui controls\n",
    "\n",
    "f1 = simplegui.create_frame(\"Obs Avoidance\", 500, 500)\n",
    "#btn_start = f1.add_button(\"Set Start\", set_start, 100)\n",
    "btn_goal = f1.add_button(\"Set Goal\", set_goal, 100)\n",
    "btn_robot = f1.add_button(\"Set Robot\", set_robot_pos, 100)\n",
    "txt_r_co = f1.add_input(\"Robot Co\", alter_co, 100)\n",
    "btn_step = f1.add_button(\"Step\", step, 100)\n",
    "\n",
    "btn_add_obs = f1.add_button(\"Add Obs\", add_obs, 100)\n",
    "\n",
    "f1.set_draw_handler(draw)\n",
    "f1.set_mouseclick_handler(click)\n",
    "\n",
    "#start simplegui\n",
    "\n",
    "f1.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the alternatives\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert below the code for alternatives: different RL methods, new assuptions, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the aspects of the performance of the algoritms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"robot_movements.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "<center><b>Figure 3. Final Robot Movements with RL</b></center>\n",
    "<center><i>Source: XXXXXX</i></center>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python UWaterloo",
   "language": "python",
   "name": "uwaterloo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
